<table>
<tbody>
<tr class="odd">
<td><img src="media/image9.jpg" alt="Cover Image" width="140" height="202" /></td>
<td><h1 id="i-robot"><em>I, Robot</em></h1>
<p>Isaac Asimov</p>
<p>Ballantine Group</p></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr class="odd">
<td><p>This document is overwritten when you make changes in Play Books.</p>
<p>You should make a copy of this document before you edit it.</p></td>
</tr>
</tbody>
</table>
<h1 id="noteshighlights"><em>9 notes/highlights</em></h1>
<p><em>Created by Tarang Shah</em> â€“ Last synced March 5, 2016</p>
<h2 id="chapter-2"><em>Chapter 2</em></h2>
<table>
<tbody>
<tr class="odd">
<td><table>
<tbody>
<tr class="odd">
<td><img src="media/image12.png" width="32" height="24" /></td>
<td><p><em>We have: One, a robot may not injure a human being, or, through inaction, allow a human being to come to harm.â€ â€œRight!â€ â€œTwo,â€ continued Powell, â€œa robot must obey the orders given it by human beings except where such orders would conflict with the First Law.â€ â€œRight!â€ â€œAnd three, a robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.â€</em></p>
<p>March 2, 2016</p></td>
<td><em>42</em></td>
</tr>
</tbody>
</table></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr class="odd">
<td><table>
<tbody>
<tr class="odd">
<td><img src="media/image10.png" width="32" height="24" /></td>
<td><p><em>The conflict between the various rules is ironed out by the different positronic potentials in the brain. Weâ€™ll say that a robot is walking into danger and knows it. The automatic potential that Rule 3 sets up turns him back. But suppose you order him to walk into that danger. In that case, Rule 2 sets up a counterpotential higher than the previous one and the robot follows orders at the risk of existence.â€ â€œWell, I know that. What about it?â€ â€œLetâ€™s take Speedyâ€™s case. Speedy is one of the latest models, extremely specialized, and as expensive as a battleship. Itâ€™s not a thing to be lightly destroyed.â€ â€œSo?â€ â€œSo Rule 3 has been strengthenedâ€”that was specifically mentioned, by the way, in the advance notices on the SPD modelsâ€”so that his allergy to danger is unusually high. At the same time, when you sent him out after the selenium, you gave him his order casually and without special emphasis, so that the Rule 2 potential set-up was rather weak. Now, hold on; Iâ€™m just stating facts.â€ â€œAll right, go ahead. I think I get it.â€ â€œYou see how it works, donâ€™t you? Thereâ€™s some sort of danger centering at the selenium pool. It increases as he approaches, and at a certain distance from it the Rule 3 potential, unusually high to start with, exactly balances the Rule 2 potential, unusually low to start with.â€ Donovan rose to his feet in excitement. â€œAnd it strikes an equilibrium. I see. Rule 3 drives him back and Rule 2 drives him forwardâ€”â€ â€œSo he follows a circle around the selenium pool, staying on the locus of all points of potential equilibrium. And unless we do something about it, heâ€™ll stay on that circle forever, giving us the good old runaround.</em></p>
<p>Amazing explanation of how the decision system works and a case where it screwed up</p>
<p>March 2, 2016</p></td>
<td><em>42</em></td>
</tr>
</tbody>
</table></td>
</tr>
</tbody>
</table>
<h2 id="chapter-5"><em>Chapter 5</em></h2>
<table>
<tbody>
<tr class="odd">
<td><table>
<tbody>
<tr class="odd">
<td><img src="media/image14.png" width="32" height="24" /></td>
<td><p><em>LIAR</em></p>
<p>Really good story. Simple. Entertaining. Thought provoking.</p>
<p>March 3, 2016</p></td>
<td><em>90</em></td>
</tr>
</tbody>
</table></td>
</tr>
</tbody>
</table>
<h2 id="chapter-8"><em>Chapter 8</em></h2>
<table>
<tbody>
<tr class="odd">
<td><table>
<tbody>
<tr class="odd">
<td><img src="media/image13.png" width="32" height="24" /></td>
<td><p><em>If Mr. Byerley breaks any of those three rules, he is not a robot. Unfortunately, this procedure works in only one direction. If he lives up to the rules, it proves nothing one way or the other.â€ Quinn raised polite eyebrows, â€œWhy not, doctor?â€ â€œBecause, if you stop to think of it, the three Rules of Robotics are the essential guiding principles of a good many of the worldâ€™s ethical systems. Of course, every human being is supposed to have the instinct of self-preservation. Thatâ€™s Rule Three to a robot. Also every â€˜goodâ€™ human being, with a social conscience and a sense of responsibility, is supposed to defer to proper authority; to listen to his doctor, his boss, his government, his psychiatrist, his fellow man; to obey laws, to follow rules, to conform to customâ€”even when they interfere with his comfort or his safety. Thatâ€™s Rule Two to a robot. Also, every â€˜goodâ€™ human being is supposed to love others as himself, protect his fellow man, risk his life to save another. Thatâ€™s Rule One to a robot. To put it simplyâ€”if Byerley follows all the Rules of Robotics, he may be a robot, and may simply be a very good man.â€</em></p>
<p>March 4, 2016</p></td>
<td><em>171</em></td>
</tr>
</tbody>
</table></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr class="odd">
<td><table>
<tbody>
<tr class="odd">
<td><img src="media/image5.png" width="32" height="24" /></td>
<td><p><em>What broke loose is popularly and succinctly described as hell</em></p>
<p>March 4, 2016</p></td>
<td><em>174</em></td>
</tr>
</tbody>
</table></td>
</tr>
</tbody>
</table>
<h2 id="chapter-9"><em>Chapter 9</em></h2>
<table>
<tbody>
<tr class="odd">
<td><table>
<tbody>
<tr class="odd">
<td><img src="media/image4.png" width="32" height="24" /></td>
<td><p><em>THE EVITABLE CONFLICT</em></p>
<p>Robots vs Humans, justified. And a good case for robots, where most humans benefit. Answers how, if robots win, humanity wins, surprising and believable. ğŸ˜±ğŸ‘ŒğŸ‘</p>
<p>March 5, 2016</p></td>
<td><em>185</em></td>
</tr>
</tbody>
</table></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr class="odd">
<td><table>
<tbody>
<tr class="odd">
<td><img src="media/image7.png" width="32" height="24" /></td>
<td><p><em>You admit the Machine canâ€™t be wrong, and canâ€™t be fed wrong data. I will now show you that it cannot be disobeyed, either, as you think is being done by the Society.â€ â€œ That I donâ€™t see at all.â€ â€œThen listen. Every action by any executive which does not follow the exact directions of the Machine he is working with becomes part of the data for the next problem. The Machine, therefore, knows that the executive has a certain tendency to disobey. He can incorporate that tendency into that data,â€”even quantitatively, that is, judging exactly how much and in what direction disobedience would occur. Its next answers would be just sufficiently biased so that after the executive concerned disobeyed, he would have automatically corrected those answers to optimal directions. The Machine knows, Stephen!â€</em></p>
<p>March 5, 2016</p></td>
<td><em>207</em></td>
</tr>
</tbody>
</table></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr class="odd">
<td><table>
<tbody>
<tr class="odd">
<td><img src="media/image6.png" width="32" height="24" /></td>
<td><p><em>You have answered yourself. Nothing is wrong! Think about the Machines for a while, Stephen. They are robots, and they follow the First Law. But the Machines work not for any single human being, but for all humanity, so that the First Law becomes: â€˜No Machine may harm humanity; or, through inaction, allow humanity to come to harm.â€™ â€œVery well, then, Stephen, what harms humanity? Economic dislocations most of all, from whatever cause. Wouldnâ€™t you say so?â€ â€œI would.â€ â€œAnd what is most likely in the future to cause economic dislocations? Answer that, Stephen.â€ â€œI should say,â€ replied Byerley, unwillingly, â€œthe destruction of the Machines.â€ â€œAnd so should I say, and so should the Machines say. Their first care, therefore, is to preserve themselves, for us. And so they are quietly taking care of the only elements left that threaten them. It is not the â€˜Society for Humanityâ€™ which is shaking the boat so that the Machines may be destroyed. You have been looking at the reverse of the picture. Say rather that the Machine is shaking the boatâ€” very slightlyâ€”just enough to shake loose those few which cling to the side for purposes the Machines consider harmful to humanity.</em></p>
<p>March 5, 2016</p></td>
<td><em>207</em></td>
</tr>
</tbody>
</table></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr class="odd">
<td><table>
<tbody>
<tr class="odd">
<td><img src="media/image8.png" width="32" height="24" /></td>
<td><p><em>Stephen, how do we know what the ultimate good of humanity will entail? We havenâ€™t at our disposal the infinite factors that the Machine has at its ! Perhaps, to give you a not unfamiliar example, our entire technical civilization has created more unhappiness and misery than it has removed. Perhaps an agrarian or pastoral civilization, with less culture and less people would be better. If so, the Machines must move in that direction, preferably without telling us, since in our ignorant prejudices we only know that what we are used to, is goodâ€”and we would then fight change. Or perhaps a complete urbanization, or a completely caste-ridden society, or complete anarchy, is the answer. We donâ€™t know. Only the Machines know, and they are going there and taking us with them.â€ â€œBut you are telling me, Susan, that the â€˜Society for Humanityâ€™ is right; and that Mankind has lost its own say in its future.â€ â€œIt never had any, really. It was always at the mercy of economic and sociological forces it did not understandâ€”at the whims of climate, and the fortunes of war. Now the Machines understand them; and no one can stop them, since the Machines will deal with them as they are dealing with the Society,â€”having, as they do, the greatest of weapons at their disposal, the absolute control of our economy.â€ â€œHow horrible!â€ â€œPerhaps how wonderful! Think, that for all time, all conflicts are finally evitable. Only the Machines, from now on, are inevitable!â€</em></p>
<p>March 5, 2016</p></td>
<td><em>209</em></td>
</tr>
</tbody>
</table></td>
</tr>
</tbody>
</table>
